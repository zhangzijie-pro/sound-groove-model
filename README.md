
---

# ğŸ™ï¸ Sound-Groove: Speaker Verification with ECAPA-TDNN

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![GitHub stars](https://img.shields.io/github/stars/zhangzijie-pro/Speaker-Verification.svg?style=social)](https://github.com/zhangzijie-pro/Speaker-Verification/stargazers)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Model%20%26%20Dataset-yellow.svg)](https://huggingface.co/zzj-pro)
![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c?logo=pytorch)
![Task](https://img.shields.io/badge/Task-Speaker%20Verification-green)

<div align="center">
  <a href="Readme_ch.md">ä¸­æ–‡æ–‡æ¡£</a> â€¢ 
  <a href="https://github.com/zhangzijie-pro/Speaker-Verification">GitHub</a> â€¢ 
  <a href="https://huggingface.co/zzj-pro">Hugging Face</a>
</div>

> A practical speaker verification system based on **ECAPA-TDNN + AAM-Softmax**, trained and evaluated on **CN-Celeb**.

---

## âœ¨ Features

- **SOTA Backbone**: ECAPA-TDNN (Res2Net + SE + Attentive Statistics Pooling)
- **Strong Discriminative Loss**: AAM-Softmax with angular margin
- **Balanced Sampling**: PK Batch Sampler (speaker-balanced)
- **Robust Evaluation**: EER, score distribution, t-SNE, Recall@K
- **Stable Inference**: Multi-crop averaging for reliable embeddings
- **Low Memory Design**: Optimized for ~6GB GPU (AMP + gradient clipping)

---

## ğŸ“‚ Project Structure

```
Sound-Groove/
â”œâ”€â”€ configs/                  # Hydra configs
â”‚   â””â”€â”€ train.yaml
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ preprocess.py         # Parallel preprocessing
â”‚   â””â”€â”€ export.py             # ONNX / MNN export + model splitting
â”œâ”€â”€ dataset/                  # Datasets & samplers
â”œâ”€â”€ models/
â”œâ”€â”€ loss_head/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio.py              # Audio loading & fbank extraction
â”‚   â”œâ”€â”€ path_utils.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ demo/                     # Gradio web demo
â”œâ”€â”€ outputs/                  # Training checkpoints & curves
â”œâ”€â”€ outputs_eval/             # Evaluation results (plots, metrics)
â”œâ”€â”€ train.py                  # Training script (Hydra)
â”œâ”€â”€ verify.py                 # Full verification evaluation
â”œâ”€â”€ compare_two_wavs.py       # Compare two audio files (PT + ONNX)
â”œâ”€â”€ README.md
â”œâ”€â”€ README_ch.md
â””â”€â”€ LICENSE
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/zhangzijie-pro/Speaker-Verification.git
cd Speaker-Verification
pip install -r requirements.txt
```

### 2. Data Preprocessing (Run once)

```bash
python scripts/preprocess.py \
    --data_dir /path/to/CN-Celeb_flac \
    --output_dir processed/cn_celeb2 \
    --n_jobs 16
```

### 3. Training

```bash
# Train with default config
python train.py

# Override parameters via command line
python train.py train.epochs=100 train.lr=5e-4 train.emb_dim=256
```

---

## ğŸ“ˆ Evaluation (Speaker Verification)

### Run full evaluation

```bash
python verify.py \
    --val_meta processed/cn_celeb2/val_meta.jsonl \
    --ckpt outputs/best.pt \
    --out_dir outputs_eval
```

**Outputs**:
- `roc.png`, `det.png`, `score_hist.png`
- `tsne.png` (speaker clustering)
- `metrics.txt` (EER, Recall@K, etc.)

---

## ğŸ¯ Single Audio Comparison (Most Used)

```bash
python compare_two_wavs.py \
    --wav1 test1.wav \
    --wav2 test2.wav \
    --ckpt outputs/export/model.onnx   # Supports ONNX
```

---

## ğŸ› ï¸ Model Export (Deployment)

```bash
# One-click export to ONNX + MNN
python scripts/export.py \
    --ckpt outputs/best.pt \
    --out_dir outputs/deploy \
    --onnx --mnn
```

**Supported deployment**:
- **ONNX Runtime** (Python / C++)
- **MNN** (Mobile / Edge)
- **TensorRT** (High-performance server)

---

## ğŸ§  Model Overview

### Backbone

- **ECAPA-TDNN**
  - Res2Net-style temporal convolutions
  - Squeeze-and-Excitation (SE)
  - Attentive Statistics Pooling
- Embedding dimension: **192 / 256**

### Loss

- **AAM-Softmax (Additive Angular Margin Softmax)**
  - Encourages large inter-speaker margins
  - Used only during training

### Embedding

- L2-normalized speaker embeddings
- Cosine similarity for verification

---

## ğŸ“Š Dataset

- **CN-Celeb**
  - ~1000 speakers
  - Highly diverse recording conditions
- Split:
  - `train`: speaker-disjoint
  - `val`: speaker-disjoint
- Features:
  - 80-dim log Mel-filterbank
  - 16kHz sampling rate

---

## ğŸ“Œ Recommended Configuration (6GB GPU)

```yaml
# configs/train.yaml
emb_dim: 256
channels: 512
lr: 1e-3
epochs: 80
crop_frames: 200          # Training
crop_frames_val: 400      # Validation
num_crops: 6
p: 32
k: 4
```

---

## ğŸ”® Future Improvements

- [x] Hydra configuration
- [x] Parallel preprocessing
- [x] ONNX / MNN export
- [ ] Noise / RIR augmentation

---

## ğŸ“œ License

This project is released under the **Apache License 2.0**.  
The CN-Celeb dataset follows its original license and usage terms.

---

## ğŸ™‹ Notes

This repository is intended for:

- Learning speaker verification systems

It is **not** an off-the-shelf commercial system.
