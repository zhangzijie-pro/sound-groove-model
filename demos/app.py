import os
import torch
import torchaudio
import gradio as gr
import numpy as np
import matplotlib.pyplot as plt
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)

sys.path.append(project_root)

from models.ecapa import ECAPA_TDNN
from utils.meters import _l2norm

# ========================= é…ç½® =========================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CKPT_PATH = "../outputs/best.pt"
SAMPLE_RATE = 16000
CROP_FRAMES = 400
NUM_CROPS = 6
SIM_THRESHOLD = 0.65

# ========================= æ¨¡å‹åŠ è½½ =========================
print("Loading model...")
ckpt = torch.load(CKPT_PATH, map_location="cpu")
model = ECAPA_TDNN(
    in_channels=80,
    channels=ckpt.get("channels", 512),
    embd_dim=256
).to(DEVICE)
model.load_state_dict(ckpt["model"], strict=True)
model.eval()
print(f"âœ… Model loaded on {DEVICE}")

# ========================= éŸ³é¢‘å¤„ç†å‡½æ•°ï¼ˆä¸å˜ï¼‰ =========================
def load_and_process_audio(audio_path: str):
    waveform, sr = torchaudio.load(audio_path)
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    if sr != SAMPLE_RATE:
        resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)
        waveform = resampler(waveform)
    fbank = torchaudio.transforms.MelSpectrogram(
        sample_rate=SAMPLE_RATE, n_fft=512, win_length=400, hop_length=160,
        n_mels=80, f_min=20, f_max=8000, norm="slaney", mel_scale="slaney"
    )(waveform.squeeze(0))
    fbank = torch.log(fbank + 1e-6).transpose(0, 1)
    return fbank

@torch.no_grad()
def extract_embedding(audio_path: str):
    feat = load_and_process_audio(audio_path)
    T = feat.size(0)
    if T <= CROP_FRAMES:
        x = feat.unsqueeze(0).to(DEVICE)
        emb = model(x).squeeze(0).cpu()
    else:
        embs = []
        for _ in range(NUM_CROPS):
            start = np.random.randint(0, T - CROP_FRAMES)
            chunk = feat[start:start + CROP_FRAMES]
            x = chunk.unsqueeze(0).to(DEVICE)
            embs.append(model(x).squeeze(0).cpu())
        emb = torch.stack(embs, 0).mean(0)
    return _l2norm(emb)

# ========================= éªŒè¯å‡½æ•° =========================
def verify_speakers(audio1, audio2):
    if audio1 is None or audio2 is None:
        return "è¯·ä¸Šä¼ ä¸¤ä¸ªéŸ³é¢‘ï¼", 0.0, None, None
    try:
        emb1 = extract_embedding(audio1)
        emb2 = extract_embedding(audio2)
        sim = float((emb1 * emb2).sum().item())
        is_same = sim > SIM_THRESHOLD
        result_text = "âœ… **åŒä¸€è¯´è¯äºº**" if is_same else "âŒ **ä¸åŒè¯´è¯äºº**"
        
        def plot_waveform(audio_path, title):
            waveform, _ = torchaudio.load(audio_path)
            waveform = waveform.mean(0).numpy()
            fig, ax = plt.subplots(figsize=(10, 3))
            ax.plot(waveform)
            ax.set_title(title)
            ax.grid(True)
            return fig
        
        return (
            f"{result_text}\n**ç›¸ä¼¼åº¦**: {sim:.4f}",
            sim,
            plot_waveform(audio1, "Speaker 1"),
            plot_waveform(audio2, "Speaker 2")
        )
    except Exception as e:
        return f"é”™è¯¯: {str(e)}", 0.0, None, None

with gr.Blocks(title="è¯´è¯äººéªŒè¯ Demo", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤ Speaker Verification åœ¨çº¿ Demo")
    gr.Markdown("**åŸºäº ECAPA-TDNN + AAM-Softmax** | æ”¯æŒéº¦å…‹é£å®æ—¶å½•éŸ³")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### è¯´è¯äºº 1")
            audio1 = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="ä¸Šä¼ æˆ–å½•éŸ³"
            )
        
        with gr.Column():
            gr.Markdown("### è¯´è¯äºº 2")
            audio2 = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="ä¸Šä¼ æˆ–å½•éŸ³"
            )
    
    with gr.Row():
        btn = gr.Button("ğŸ” å¼€å§‹éªŒè¯", variant="primary", size="large")
    
    with gr.Row():
        with gr.Column(scale=2):
            result = gr.Markdown(label="éªŒè¯ç»“æœ", value="ç­‰å¾…éªŒè¯...")
        with gr.Column():
            score = gr.Number(label="ç›¸ä¼¼åº¦åˆ†æ•°", value=0.0)
    
    with gr.Row():
        waveform1 = gr.Plot(label="è¯´è¯äºº 1 æ³¢å½¢")
        waveform2 = gr.Plot(label="è¯´è¯äºº 2 æ³¢å½¢")
    
    gr.Markdown("---\n**ä½¿ç”¨è¯´æ˜**ï¼š\n"
                "1. ä¸Šä¼ ä¸¤ä¸ª .wav æ–‡ä»¶ï¼ˆæˆ–ç›´æ¥å½•éŸ³ï¼‰\n"
                "2. ç‚¹å‡»ã€Œå¼€å§‹éªŒè¯ã€\n"
                "3. ç›¸ä¼¼åº¦ > 0.65 åˆ¤å®šä¸ºåŒä¸€è¯´è¯äºº")

    # ç»‘å®š
    btn.click(
        fn=verify_speakers,
        inputs=[audio1, audio2],
        outputs=[result, score, waveform1, waveform2]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=True,
        debug=True
    )